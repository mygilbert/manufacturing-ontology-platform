# 관계 발견 엔진 (Relationship Discovery Engine)

## 1. 개요

공정 데이터에서 **숨겨진 관계**를 자동으로 발견하는 분석 엔진입니다.

### 위치
```
analytics/src/relationship_discovery/
├── __init__.py              # 모듈 진입점
├── config.py                # 설정 클래스
├── correlation_analyzer.py  # 상관분석
├── causality_analyzer.py    # 인과분석
├── pattern_detector.py      # 패턴 탐지
├── relationship_store.py    # 온톨로지 저장
├── discovery_pipeline.py    # 통합 파이프라인
└── expert_knowledge_loader.py # 도메인 지식 로더
```

## 2. 분석 방법론

### 2.1 상관분석 (Correlation Analysis)

파라미터 간 **선형/비선형 상관관계**를 분석합니다.

```python
from relationship_discovery import CorrelationAnalyzer
from relationship_discovery.config import CorrelationConfig

config = CorrelationConfig(
    methods=['pearson', 'spearman'],  # 분석 방법
    min_correlation=0.5,              # 최소 상관계수
    max_p_value=0.05,                 # 유의수준
    max_lag=10                        # 최대 시차 (Cross-correlation)
)

analyzer = CorrelationAnalyzer(config)
results = analyzer.analyze(
    data=df,
    columns=['temperature', 'pressure', 'flow_rate'],
    timestamp_col='timestamp'
)
```

**결과 예시**:
```
temperature ↔ pressure: r=0.72 (Pearson)
pressure ↔ flow_rate: r=0.65, lag=2s (Cross-correlation)
```

### 2.2 인과분석 (Causality Analysis)

**Granger Causality**와 **Transfer Entropy**로 인과관계를 분석합니다.

```python
from relationship_discovery import CausalityAnalyzer
from relationship_discovery.config import CausalityConfig

config = CausalityConfig(
    max_lag=10,              # 최대 시차
    significance_level=0.05, # 유의수준
    use_granger=True,        # Granger Causality 사용
    use_transfer_entropy=True # Transfer Entropy 사용
)

analyzer = CausalityAnalyzer(config)
results = analyzer.analyze(data=df, columns=['A', 'B', 'C'])

# 양방향 관계 (피드백 루프) 찾기
bidirectional = analyzer.find_bidirectional()

# 인과 그래프 생성
graph = analyzer.get_causal_graph()
# {'A': ['B', 'C'], 'B': ['C']}
```

**Granger Causality 원리**:
> "X가 Y를 Granger-cause한다"는 것은 X의 과거 값이 Y의 미래 값 예측에 도움이 된다는 의미

### 2.3 패턴 탐지 (Pattern Detection)

**Sequential Pattern Mining**과 **Association Rules**로 이벤트 패턴을 발견합니다.

```python
from relationship_discovery import PatternDetector
from relationship_discovery.config import PatternConfig

config = PatternConfig(
    min_support=0.1,       # 최소 지지도
    min_confidence=0.5,    # 최소 신뢰도
    max_pattern_length=5,  # 최대 패턴 길이
    time_window_seconds=60 # 이벤트 윈도우
)

detector = PatternDetector(config)
patterns = detector.detect(
    event_data=events_df,
    timestamp_col='timestamp',
    event_col='event_type'
)

# 근본원인 패턴 찾기
root_causes = detector.find_root_cause_patterns(
    target_events=['ALARM_CRITICAL']
)
```

**결과 예시**:
```
[TEMP_HIGH, PRESSURE_HIGH] → ALARM (support: 0.15, confidence: 0.82)
RF_ADJUST → ETCH_RATE_OOS (confidence: 0.91)
```

## 3. 통합 파이프라인

전체 분석을 한 번에 실행하는 파이프라인입니다.

```python
from relationship_discovery import DiscoveryPipeline, DiscoveryConfig

# 설정
config = DiscoveryConfig()
config.correlation.min_correlation = 0.3
config.causality.max_lag = 10
config.pattern.min_confidence = 0.5

# 파이프라인 실행
pipeline = DiscoveryPipeline(config)
relationships = pipeline.discover_all(
    pv_data=pv_df,           # 공정 변수 시계열
    event_data=event_df,     # 이벤트 로그
    pv_columns=['temp', 'pressure', 'vibration'],
    timestamp_col='timestamp',
    event_col='event_type'
)

# 리포트 생성
print(pipeline.generate_report())

# 결과 내보내기
pipeline.export_results('report.html', format='html')
pipeline.export_results('relationships.csv', format='csv')
pipeline.export_results('relationships.json', format='json')
```

## 4. 온톨로지 저장

발견된 관계를 PostgreSQL + Apache AGE에 저장합니다.

```python
from relationship_discovery import RelationshipStore

store = RelationshipStore(connection_string)

# 관계 저장 (pending 상태)
store.save_pending(relationships)

# 도메인 전문가 검증 후 확정
store.verify_relationship(
    relationship_id='REL-001',
    verified_by='engineer@company.com',
    notes='현장 확인 완료'
)

# 거부
store.reject_relationship(
    relationship_id='REL-002',
    rejected_by='engineer@company.com',
    reason='설비 연결 없음'
)
```

## 5. 테스트 실행

### 합성 데이터 테스트
```bash
cd analytics
python scripts/test_relationship_discovery.py
```

### 실제 샘플 데이터 테스트
```bash
python scripts/test_real_data_discovery.py
```

### 테스트 결과 확인
- HTML 리포트: `analytics/results/relationship_report.html`
- CSV: `analytics/results/relationships.csv`

## 6. 설정 파라미터 가이드

### CorrelationConfig

| 파라미터 | 기본값 | 설명 |
|---------|-------|------|
| methods | ['pearson', 'spearman'] | 상관분석 방법 |
| min_correlation | 0.5 | 최소 상관계수 임계값 |
| max_p_value | 0.05 | 최대 p-value (유의수준) |
| max_lag | 10 | Cross-correlation 최대 시차 |
| min_samples | 100 | 분석에 필요한 최소 샘플 수 |

### CausalityConfig

| 파라미터 | 기본값 | 설명 |
|---------|-------|------|
| max_lag | 10 | 인과분석 최대 시차 |
| significance_level | 0.05 | Granger 테스트 유의수준 |
| use_granger | True | Granger Causality 사용 |
| use_transfer_entropy | True | Transfer Entropy 사용 |
| min_samples | 500 | 최소 샘플 수 |

### PatternConfig

| 파라미터 | 기본값 | 설명 |
|---------|-------|------|
| min_support | 0.1 | 최소 지지도 |
| min_confidence | 0.5 | 최소 신뢰도 |
| max_pattern_length | 5 | 최대 패턴 길이 |
| time_window_seconds | 60 | 이벤트 그룹핑 윈도우 |

## 7. 주의사항

### 상관관계 ≠ 인과관계
- 상관분석 결과는 **연관성**만 보여줄 뿐, 인과관계를 증명하지 않음
- 인과분석 결과도 **통계적 인과**일 뿐, 물리적 인과와 다를 수 있음
- 반드시 **도메인 전문가 검증** 필요

### 데이터 품질 요구사항
- 시계열 데이터는 **등간격** 샘플링 권장
- 결측치가 많으면 분석 품질 저하
- 최소 1,000개 이상의 샘플 권장

### 성능 고려사항
- 파라미터 수가 많으면 O(n^2) 복잡도
- 대용량 데이터는 샘플링 또는 배치 처리 필요
